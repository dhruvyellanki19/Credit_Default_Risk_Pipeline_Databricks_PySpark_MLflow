{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575b6884-b92c-4ac3-8600-a97b879e8d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"credit_risk_eda\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe856ad-a6b1-4eff-8f3b-e039b0d53b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "application_df=spark.table(\"workspace.credit_risk_data_delta.application_train\")\n",
    "application_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760527f3-6065-4df5-9a5f-0091a59eb0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(application_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2f5cc6-693e-4e7e-a4cb-14abe9f1252e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(application_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37595a4c-d169-4159-b302-aaeb93823f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Target Imbalance:\n",
    "The `TARGET` variable has a mean of approximately **0.08**, confirming a **strong class imbalance** — only ~8% of applicants default.  \n",
    "This requires:\n",
    "- **Stratified sampling** during train-test split  \n",
    "- Evaluation metrics like **AUC**, **F1**, or **log loss** over accuracy\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Missing Values:\n",
    "Many columns — especially **apartment-level features** and **housing metadata** — have significantly fewer entries than 307,511, suggesting heavy missingness.  \n",
    "Action Plan:\n",
    "- Use null percentage summaries to decide on **imputation or removal**  \n",
    "- Prioritize important columns for retention during feature engineering \n",
    "---\n",
    "\n",
    "#### 3. Applicant Demographics & Behavior:\n",
    "\n",
    "- **Age (`DAYS_BIRTH`)**: Mean age is ~44 years (16037 / 365)  \n",
    "- **Employment (`DAYS_EMPLOYED`)**: Contains unrealistic values (e.g., 365243), which likely indicate retirees or placeholder codes — needs cleaning\n",
    "- **Income & Credit**:  \n",
    "  - `AMT_INCOME_TOTAL` and `AMT_CREDIT` show **high variance** and **extreme outliers**  \n",
    "  - These features are ideal candidates for **log transformation** or **binning**\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Data Consistency & Encoding Clues:\n",
    "Some categorical features such as `NAME_CONTRACT_TYPE`, `CODE_GENDER`, `OCCUPATION_TYPE`, etc., contain **ambiguous entries** like `'XNA'`.  \n",
    "These will require:\n",
    "- **Cleaning and consolidation**\n",
    "- Proper encoding during (e.g., one-hot or target encoding)\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Engineered External Scores:\n",
    "Features `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`:\n",
    "- Appear to be **pre-normalized** (values in 0–1 range)\n",
    "- Typically **strong predictors** in credit risk modeling  \n",
    "Must visualize their correlation with `TARGET` in `01_eda.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Document Flags and Count Features:\n",
    "Flags like `FLAG_DOCUMENT_3`, `FLAG_PHONE`, etc.:\n",
    "- Are mostly **binary indicators** or **low-range counts**\n",
    "- Can be used directly or **combined into composite features** (e.g., total documents submitted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bec5d3-39d6-4536-a0d0-0535c0b2223b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the number of Rows and Columns\n",
    "num_rows = application_df.count()\n",
    "num_cols = len(application_df.columns)\n",
    "print(f\"({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a340a3-3231-414f-aa2f-35b8b972e8ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(dict(application_df.dtypes).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95dbc5a3-77b2-438f-bbde-e4b1f3a1aea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter only string (categorical) columns\n",
    "categorical_cols = [field.name for field in application_df.schema.fields if field.dataType.simpleString() == 'string']\n",
    "\n",
    "# Show number of unique values for each categorical column\n",
    "for col_name in categorical_cols:\n",
    "    count = application_df.select(col_name).distinct().count()\n",
    "    print(f\"{col_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6359a665-74fb-4d3e-a1f7-ee95a5411912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_exploratory_data_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
